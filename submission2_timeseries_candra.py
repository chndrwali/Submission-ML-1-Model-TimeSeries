# -*- coding: utf-8 -*-
"""Submission2_TimeSeries_Candra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qQMx0AjcxMUEWw7a4unt3PoczcQB1OQY

# Submission 2 Membuat Model Machine Learning dengan Data Time Series
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.layers import Dense, LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

dataset = pd.read_csv('/content/AMZN_2006-01-01_to_2018-01-01.csv')
dataset

dataset.isnull().sum()

dates = dataset['Date'].values
selling = dataset['High'].values

plt.figure(figsize=(15,5))
plt.plot(dates, selling)
plt.title('Selling Average',
          fontsize=20);

dataset['Date'] = pd.to_datetime(dataset['Date'])
dataset = dataset.set_index('Date')

selling = selling.reshape(-1,1)
selling

scaler = MinMaxScaler()
selling = scaler.fit_transform(selling)

X_train, X_test, y_train, y_test = train_test_split(selling, dates,
                                                    test_size=0.2,
                                                    shuffle=False)

print('Total Data Train : ',len(X_train))
print('Total Data Validation : ',len(X_test))

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(X_train, window_size=60, batch_size=64, shuffle_buffer=1000)
test_set  = windowed_dataset(X_test, window_size=128, batch_size=64, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(50, return_sequences=True, input_shape = [None, 1]),
  tf.keras.layers.LSTM(50),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

threshold_mae = (selling.max() - selling.min()) * 10/100
print(threshold_mae)

class myCallback(tf.keras.callbacks.Callback):
  def __init__(self, min_epochs=10):
    self.min_epochs = min_epochs
    self.epochs_passed = 0

  def on_epoch_end(self, epoch, logs={}):
    self.epochs_passed += 1

    if self.epochs_passed >= self.min_epochs and logs.get('mae') < threshold_mae:
      print(f"MAE < 10% after {self.epochs_passed} epochs.")
      self.model.stop_training = True

callbacks = myCallback(min_epochs=20)

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set,
                    epochs=100,
                    validation_data=test_set,
                    verbose=2,
                    callbacks=[callbacks])

plt.figure(figsize=(10, 6))
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Training and Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()